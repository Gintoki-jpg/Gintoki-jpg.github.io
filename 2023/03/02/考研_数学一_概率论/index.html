

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/bg/logo.png">
  <link rel="icon" href="/img/bg/logo.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
    <meta http-equiv="Content-Security-Policy" content="upgrade-insecure-requests">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="杨再俨">
  <meta name="keywords" content="">
  
    <meta name="description" content="24考研数学一概率论复习总结">
<meta property="og:type" content="article">
<meta property="og:title" content="考研_数学一_概率论">
<meta property="og:url" content="https://gintoki-jpg.github.io/2023/03/02/%E8%80%83%E7%A0%94_%E6%95%B0%E5%AD%A6%E4%B8%80_%E6%A6%82%E7%8E%87%E8%AE%BA/index.html">
<meta property="og:site_name" content="Tintoki_blog">
<meta property="og:description" content="24考研数学一概率论复习总结">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://gintoki-jpg.github.io/img/bg/study.jpg">
<meta property="article:published_time" content="2023-03-02T02:01:00.000Z">
<meta property="article:modified_time" content="2023-08-07T08:25:27.905Z">
<meta property="article:author" content="YangZaiyan">
<meta property="article:tag" content="概率论与数理统计">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://gintoki-jpg.github.io/img/bg/study.jpg">
  
  
  
  <title>考研_数学一_概率论 - Tintoki_blog</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"gintoki-jpg.github.io","root":"/","version":"1.9.1","typing":{"enable":false,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml"};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>

  
<meta name="generator" content="Hexo 6.3.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Tintoki_blog</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                首页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                归档
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                分类
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                关于
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              &nbsp;<i class="iconfont icon-search"></i>&nbsp;
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/bg/bg1.jpg') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle">考研_数学一_概率论</span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2023-03-02 10:01" pubdate>
          2023年3月2日 上午
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          19k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          161 分钟
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">考研_数学一_概率论</h1>
            
            <div class="markdown-body">
              
              <hr>
<p>2023&#x2F;3&#x2F;2 10:01 大三下学期的专业课对概率论的要求越来越高了，所以这段时间决定概率论开始看视频复习；</p>
<p>2023&#x2F;3&#x2F;6 16:41 个人感觉概率论不需要再像高数一样完整的把视频看完（本身也有基础，况且宋浩的课程并不是严格按照考研大纲来的），直接跟着教材原有的笔记和拓展的笔记过一遍即可，或者可以直接上手张宇的概率论30讲；</p>
<p>2023&#x2F;3&#x2F;6 21:20 现在尝试直接上手余丙森的概率论与数理统计的讲义，视频什么的暂时不用看了；</p>
<p>2023&#x2F;3&#x2F;6 23:14 余丙森的教材用来速刷还是很不错的，只是感觉知识点的讲解不是很详细，可能因为是考研书籍吧，更多的还是注重于刷题过程中掌握知识点；</p>
<p>2023&#x2F;3&#x2F;12 19:27 本来想尝试把余丙森的教材换成张宇的（本以为张宇的教材讲的更细致一点，知识点的过渡也比较自然），但是使用之后发现概率论这门课确实越精简越好，所以还是换回余丙森的教材；现阶段不想刷题就不刷题吧，按照自己喜欢的复习方式来就OK，简单过一遍知识点也没问题；</p>
<p>2023&#x2F;3&#x2F;27 21:46 本来想的是直接硬啃书，但是发现到后面需要理解概念的时候硬啃书难度较大，还是需要看视频；</p>
<p>2023&#x2F;3&#x2F;28 20:08 张宇的概率论视频可以跟，边听边做笔记，听完之后对每一小节做总结，可以把<del>余丙森的教材作为预习</del>，张宇视频作为理解，最后的总结作为吸收，分<del>三个阶段</del>学习概率论；</p>
<p>2023&#x2F;3&#x2F;30 10:34 概率论和线代没必要把书上的知识点总结的仔仔细细，只需要按照之前期末复习的笔记和视频课的重点总结关键的信息即可（高数是因为必须把基础知识掌握牢固），张宇的课和教材都很好，所以直接放弃余丙森的书，看完张宇教材之后结合重点整理到blog即可；</p>
<p>2023&#x2F;4&#x2F;20 23:38 现在需要做的就是停下来，把第二章和第三章的例题开始刷起来巩固知识点，同时高数也应当继续复习；</p>
<p>2023&#x2F;5&#x2F;15 12:25 第四讲的习题未全部完成，第五讲和第六讲仅总结了教材知识点并未刷题（不要过于纠结某些知识点的理解，会做题就行，会做题之后再理解会轻松的多）；</p>
<p>2023&#x2F;5&#x2F;15 17:34 书本知识点总结完毕，将高数和线代知识点总结完毕后进行刷题巩固；</p>
<p>2023&#x2F;6&#x2F;18 19:46 不要忘了概率论之前整理的笔记，对于一些很难搞清楚的概念之类的可以参考笔记（毕竟笔记也是当时耗费大量时间整理的）；</p>
<p>2023&#x2F;7&#x2F;18 15:41 今天复习第五讲的时候再次用到了之前整理的笔记，概率论当时整理的笔记真的是很好的，一定要整理下来！！！ 现在(16:59)已经基本把之前大二学习概率论时候的笔记整理完毕，只剩下最后区间估计的知识点因为涉及做题过程中理解，所以暂时先不整理，之后有需要可以回过头看看。现在基本上没什么疑惑点了，其实只要搞清楚概率论和数理统计两门学科之间的关系就能很容易的搞清楚这六章知识点的脉络。相对来说第五讲作为过渡性知识点会比较简单，而第六讲作为最终集大成者，基本上是考研题的大题所在，需要花时间理解和刷题掌握；</p>
<hr>
<h1 id="一、随机事件与概率"><a href="#一、随机事件与概率" class="headerlink" title="一、随机事件与概率"></a>一、随机事件与概率</h1><hr>
<blockquote>
<p>Q：概率论和数理统计的区别是什么？为什么常常将其合称为“概率统计”？</p>
</blockquote>
<p>A：</p>
<ul>
<li><p>概率论研究的是一个白箱子，你知道这个箱子的构造（里面有几个红球、几个白球，也就是所谓的分布函数），然后计算下一个摸出来的球是红球的概率；</p>
</li>
<li><p>数理统计面对的是一个黑箱子，你只看得到每次摸出来的是红球还是白球，然后需要猜测这个黑箱子的内部结构，例如红球和白球的比例是多少？（参数估计）能不能认为红球40%，白球60%？（假设检验）；</p>
</li>
</ul>
<p><img src="/images/image-20230406225054551.png" srcset="/img/loading.gif" lazyload></p>
<p>概率论中的许多定理与结论，如大数定理、中心极限定理等保证了统计推断的合理性。做统计推断一般都需要对黑箱子做各种各样的假设，这些假设都是概率模型，统计推断实际上就是在估计概率模型的参数；</p>
<p>换句话说，概率论研究概率模型，数理统计挑选概率模型；</p>
<hr>
<h2 id="1-基本概念"><a href="#1-基本概念" class="headerlink" title="1.基本概念"></a>1.基本概念</h2><h3 id="1-1-随机试验"><a href="#1-1-随机试验" class="headerlink" title="1.1 随机试验"></a>1.1 随机试验</h3><p>称一个试验为随机试验如果满足：</p>
<p>（1）试验可以在相同的条件下重复进行；<br>（2）试验所有可能结果是明确可知道的，并且不止一个；<br>（3）每一次试验会出现哪一个结果，事先并不能确定；</p>
<blockquote>
<p>抛硬币就是就是一个随机试验；</p>
<p>如果不能确切知道某一随机试验的全部可能结果，但是可以知道该结果不会超出某个值（这个范围甚至可以是(-∞，+∞)），此时也符合第二条件；</p>
</blockquote>
<h3 id="1-2-随机事件"><a href="#1-2-随机事件" class="headerlink" title="1.2 随机事件"></a>1.2 随机事件</h3><p>在随机试验中，可能出现也可能不出现的结果称为随机事件，简称为事件；</p>
<ul>
<li>每次试验中一定会发生的事件称为必然事件，记为全集Ω；</li>
<li>每次试验中一定不发生的事件称为不可能事件，记为空集；</li>
</ul>
<h3 id="1-3-样本空间"><a href="#1-3-样本空间" class="headerlink" title="1.3 样本空间"></a>1.3 样本空间</h3><p>随机试验的每一个不可再分的结果称为一个<code>样本点</code>（或基本事件），记为ω；</p>
<p>由样本点全体组成的集合称为<code>样本空间</code>（或基本事件空间），记为Ω；</p>
<blockquote>
<p>抛硬币的样本空间Ω&#x3D;{正面，反面}，其基本事件为w<del>1</del>&#x3D;{正面}，w<del>2</del>&#x3D;{反面}；</p>
</blockquote>
<h2 id="2-事件的关系和运算"><a href="#2-事件的关系和运算" class="headerlink" title="2.事件的关系和运算"></a>2.事件的关系和运算</h2><p>概率论中一个极其重要的基本功就是将题目中所给的基本事件用符号表示，这决定了后续解题的基础 – 一定注意是对基本事件进行符号化表示，比如令事件A为“抽到正品且通过验收”则这种设法是不妥当的，应当设事件B为“抽到正品”，事件C为“通过验收”。</p>
<h3 id="2-1-定义"><a href="#2-1-定义" class="headerlink" title="2.1 定义"></a>2.1 定义</h3><blockquote>
<p>事件由集合定义，因此事件的运算本质上就是集合的运算；</p>
<p>事件的关系主要有：包含、相等、相容、互斥和对立；</p>
<p>事件的运算主要有：和，差，积；</p>
</blockquote>
<p>（1）包含：如果事件A发生必然导致事件B发生，则称事件B包含事件A；</p>
<p>（2）相等：如果事件B包含事件A同时事件A包含事件B，则称事件A和事件B相等；</p>
<p>（3）事件的积&#x2F;交：如果事件C是“事件A和事件B同时发生”的事件，则称事件C是事件A和B的积&#x2F;交 – 可推出，A∩空集&#x3D;空集，A∩Ω&#x3D;A；</p>
<p>（4）相容：如果事件A和事件B的积&#x2F;交不为空集，则称事件A和事件B相容；</p>
<p>（5）互斥：如果事件A和事件B的积&#x2F;交为空集，则称事件A和事件B不相容&#x2F;互斥，如果一堆事件中的任意两个事件都互斥，则称这些事件是两两互斥的；</p>
<p>（6）事件的和&#x2F;并：如果事件C是“事件A和事件B至少有一个发生”的事件，则称事件C是事件A和B的和&#x2F;并 – 这个概念很重要，可推出A∪空集&#x3D;A，A∪Ω&#x3D;Ω；</p>
<p>（7）差事件：如果事件C是“事件A发生但事件B不发生”的事件，则称事件C是事件A与B的差时间，记为A-B；</p>
<blockquote>
<p>根据差事件的定义有如下两个性质成立</p>
<p><img src="/images/image-20230412191713158.png" srcset="/img/loading.gif" lazyload></p>
</blockquote>
<p>（8）逆事件&#x2F;对立事件：如果事件是“事件A不发生”的事件，则称事件C是事件A的逆事件或对立事件；</p>
<p><img src="/images/image-20230411205251758.png" srcset="/img/loading.gif" lazyload></p>
<blockquote>
<ul>
<li>事件的运算中有差运算A-B，但是没有A+B这种运算！！！和事件使用的是∪符号而不是+符号！！！</li>
<li><code>事件运算先后顺序约定为：逆运算、积运算，最后是和运算或差运算</code>；</li>
</ul>
</blockquote>
<h3 id="2-2-运算法则"><a href="#2-2-运算法则" class="headerlink" title="2.2 运算法则"></a>2.2 运算法则</h3><p><img src="/images/image-20230411205625402.png" srcset="/img/loading.gif" lazyload></p>
<blockquote>
<ul>
<li><p>这里列出的运算法则是完全正确的，而其他未列出的运算法则必须以此推导，不能理所当然的认为存在；</p>
</li>
<li><p>事件的运算和数的运算是存在不同的！！！所以一定要注意不要理所当然的将数的运算法则类比到事件上，如下三个推导均是错误的</p>
</li>
</ul>
<p><img src="/images/image-20230611161328876.png" srcset="/img/loading.gif" lazyload></p>
</blockquote>
<h2 id="3-概率的定义"><a href="#3-概率的定义" class="headerlink" title="3.概率的定义"></a>3.概率的定义</h2><p>概率的公理化定义如下：</p>
<p><img src="/images/image-20230411205923539.png" srcset="/img/loading.gif" lazyload></p>
<blockquote>
<ul>
<li>概率P(·)是事件的函数，输入为事件，输出为事件发生的概率；</li>
<li>概率的公理化定义并不能应用于计算，常用于该定义判断某事件函数P(·)是否是概率；</li>
</ul>
</blockquote>
<h2 id="4-古典概型和几何概型"><a href="#4-古典概型和几何概型" class="headerlink" title="4.古典概型和几何概型"></a>4.古典概型和几何概型</h2><h3 id="4-1-古典概型"><a href="#4-1-古典概型" class="headerlink" title="4.1 古典概型"></a>4.1 古典概型</h3><p>称随机试验的概率模型为古典概型，如果其样本空间满足：</p>
<ul>
<li>只有有限个样本点（如抛硬币、掷色子）；</li>
<li>每个样本点发生的可能性都一样（等可能性）；</li>
</ul>
<p>如果古典概型的基本事件总数为n，事件A包含k个基本事件，则事件A发生的概率为</p>
<p><img src="/images/image-20230411210350777.png" srcset="/img/loading.gif" lazyload></p>
<p>关于古典概型的题型，主要有以下几种考点</p>
<p><img src="/images/image-20230411210812045.png" srcset="/img/loading.gif" lazyload></p>
<blockquote>
<ul>
<li>关于排列和组合，并不需要理解的特别透彻（这很容易导致做题的时候麻木，只需要知道组合是排列除以组内部的顺序即可），可以用三个水果选两个来画图直观理解；</li>
<li>无论是随机分配问题还是随机抽样问题，本质上还是借助古典概型的定义来求概率，即P(事件A)&#x3D;基本事件数&#x2F;基本事件总数，随机分配和随机抽样问题的区别就在于基本事件总数的计算方式不同；</li>
<li>这种题的难度都不会很大，不要再像高中一样去背什么捆绑法之类的，简单能够理解就行；</li>
<li>关于排列组合，如下比较重要的性质需要记忆（可加快某些计算的速度）<br><img src="/images/image-20230718155724036.png" srcset="/img/loading.gif" lazyload></li>
</ul>
</blockquote>
<h3 id="4-2-几何概型"><a href="#4-2-几何概型" class="headerlink" title="4.2 几何概型"></a>4.2 几何概型</h3><p>称随机试验的概率模型为几何概型，如果其样本空间满足：</p>
<ul>
<li>样本空间Ω是一个可度量的有界区域（可度量指的是长度或面积）；</li>
<li>每个样本点发生的可能性一样；</li>
</ul>
<p><img src="/images/image-20230411210630471.png" srcset="/img/loading.gif" lazyload></p>
<blockquote>
<p>古典概型与几何概型的区别：基本事件有限、等可能发生的随机试验为古典概型；基本事件无限且具有几何度量、等可能发生的随机试验为几何概型；</p>
</blockquote>
<h2 id="5-重要公式求概率"><a href="#5-重要公式求概率" class="headerlink" title="5.重要公式求概率"></a>5.重要公式求概率</h2><h3 id="5-1-概率的性质"><a href="#5-1-概率的性质" class="headerlink" title="5.1 概率的性质"></a>5.1 概率的性质</h3><p>（1）有界性：对于任一事件A，有0&lt;&#x3D;P(A)&lt;&#x3D;1</p>
<blockquote>
<ul>
<li><p>不可能事件的概率为0，但是概率为0的事件不一定是不可能事件；</p>
</li>
<li><p>必然事件的概率为1，但是概率为1的事件不一定是必然事件；</p>
</li>
</ul>
</blockquote>
<p>（2）<code>单调性</code>：</p>
<p><img src="/images/image-20230411211137006.png" srcset="/img/loading.gif" lazyload></p>
<p>单调性在解题过程中尤其重要，概率公式的变形主要有以下几种</p>
<p><img src="/images/image-20230411211227288.png" srcset="/img/loading.gif" lazyload></p>
<h3 id="5-2-重要公式"><a href="#5-2-重要公式" class="headerlink" title="5.2 重要公式"></a>5.2 重要公式</h3><p>（所有难以理解的概率公式都可以使用文氏图的方式画出来 – P(A)表示集合A的面积占全集Ω面积的比率）</p>
<h4 id="5-2-1-逆事件概率公式"><a href="#5-2-1-逆事件概率公式" class="headerlink" title="5.2.1 逆事件概率公式"></a>5.2.1 逆事件概率公式</h4><p><img src="/images/image-20230411211352275.png" srcset="/img/loading.gif" lazyload></p>
<blockquote>
<ul>
<li>逆事件概率公式主要提供了一种解题的思想方法；</li>
</ul>
</blockquote>
<h4 id="5-2-2-加法公式"><a href="#5-2-2-加法公式" class="headerlink" title="5.2.2 加法公式"></a>5.2.2 加法公式</h4><p><img src="/images/image-20230411211434662.png" srcset="/img/loading.gif" lazyload></p>
<blockquote>
<p>注意：</p>
<ul>
<li><p>概率有加法对应但是事件的运算可没有</p>
</li>
<li><p><img src="/images/image-20230411211504666.png" srcset="/img/loading.gif" lazyload></p>
</li>
<li><p>另外，对于大于3个以上事件的概率加法公式，也有记忆的必要，这里给出通式</p>
</li>
</ul>
<p><img src="/images/image-20230611222131474.png" srcset="/img/loading.gif" lazyload></p>
</blockquote>
<h4 id="5-2-3-减法公式"><a href="#5-2-3-减法公式" class="headerlink" title="5.2.3 减法公式"></a>5.2.3 减法公式</h4><p><img src="/images/image-20230411211543621.png" srcset="/img/loading.gif" lazyload></p>
<blockquote>
<ul>
<li>P(ABC^-^)&#x3D;P(AB)-P(ABC) – 令AB&#x3D;P即可推出；</li>
<li>注意P(AB^-^)!&#x3D;P(A)-P(B)，因为<img src="/images/image-20230710155604888.png" srcset="/img/loading.gif" lazyload></li>
</ul>
</blockquote>
<h4 id="5-2-4-条件概率公式"><a href="#5-2-4-条件概率公式" class="headerlink" title="5.2.4 条件概率公式"></a>5.2.4 条件概率公式</h4><p><img src="/images/image-20230411211645995.png" srcset="/img/loading.gif" lazyload></p>
<blockquote>
<p>注意：</p>
<p><img src="/images/image-20230411211800947.png" srcset="/img/loading.gif" lazyload></p>
</blockquote>
<h4 id="5-2-5-乘法公式"><a href="#5-2-5-乘法公式" class="headerlink" title="5.2.5 乘法公式"></a>5.2.5 乘法公式</h4><p><img src="/images/image-20230411211857139.png" srcset="/img/loading.gif" lazyload></p>
<h4 id="5-2-6-全概率公式"><a href="#5-2-6-全概率公式" class="headerlink" title="5.2.6 全概率公式"></a>5.2.6 全概率公式</h4><p>全概率公式又称为全集分解公式，是一种“由因导果”的思想；</p>
<p><img src="/images/image-20230411211943499.png" srcset="/img/loading.gif" lazyload></p>
<blockquote>
<p>全概率公式的特点是，某事件的发生是可能在多种条件下的，而这些条件一起构成了一个完备事件组（当然这些条件可以是与该事件没啥关系的事件，比如“(我去游泳|小明吃饭了)”，但是一旦两者以条件概率的形式写出来则两者就有了条件的这种关系）</p>
</blockquote>
<h4 id="5-2-7-贝叶斯公式"><a href="#5-2-7-贝叶斯公式" class="headerlink" title="5.2.7 贝叶斯公式"></a>5.2.7 贝叶斯公式</h4><p>贝叶斯公式又称为逆概率公式，是一种“由果导因”的思想，即已知事件B发生，求事件A<del>j</del>发生的概率</p>
<p><img src="/images/image-20230411212206908.png" srcset="/img/loading.gif" lazyload></p>
<blockquote>
<ul>
<li>注意P(B)和P(B|A)的区别与联系对理解贝叶斯公式有很大作用，P(B)是在样本空间Ω下计算的事件B发生的概率，而P(B|A)是在已知事件A发生的条件下（也就是样本空间现在缩减为A）计算的；</li>
<li>贝叶斯的题型解题，两个无脑的步骤<ul>
<li>基本事件变量化；</li>
<li>由已知条件求解所有基本事件的概率；</li>
</ul>
</li>
</ul>
</blockquote>
<h3 id="5-3-常用不等式"><a href="#5-3-常用不等式" class="headerlink" title="5.3 常用不等式"></a>5.3 常用不等式</h3><p>概率的等量关系如加法公式、减法公式等重要公式前面已经介绍过这里不再赘述，这里主要总结常用的概率不等式</p>
<p><img src="/images/image-20230422163540306.png" srcset="/img/loading.gif" lazyload></p>
<h2 id="6-事件的独立性"><a href="#6-事件的独立性" class="headerlink" title="6.事件的独立性"></a>6.事件的独立性</h2><p>设A,B为两个事件，如果P(AB)&#x3D;P(A)P(B)则称事件A与事件B相互独立，简称事件A和事件B独立：</p>
<blockquote>
<ul>
<li>不可能事件与任何事件(包括其他不可能事件)相互独立；</li>
<li>事件A与事件B相互独立则A的对立事件与B也相互独立；</li>
</ul>
</blockquote>
<p><img src="/images/image-20230411212844176.png" srcset="/img/loading.gif" lazyload></p>
<blockquote>
<ul>
<li>互不相容&#x2F;互斥事件可以简化并事件的概率计算：P(A<del>1</del>∪A<del>2</del>∪…∪A<del>n</del>)&#x3D;P(A<del>1</del>)+P(A<del>2</del>)+…+P(A<del>n</del>)；</li>
<li>相互独立可以简化交事件的概率计算：P(A<del>1</del>∩A<del>2</del>∩…∩A<del>n</del>)&#x3D;P(A<del>1</del>)P(A<del>2</del>)…P(A<del>n</del>)；</li>
</ul>
</blockquote>
<h1 id="二、一维随机变量及其分布"><a href="#二、一维随机变量及其分布" class="headerlink" title="二、一维随机变量及其分布"></a>二、一维随机变量及其分布</h1><h2 id="1-基本概念-1"><a href="#1-基本概念-1" class="headerlink" title="1.基本概念"></a>1.基本概念</h2><p>随机变量的意义在于统一和量化<code>事件</code>（原本使用集合来定义事件，这导致很难用数学方法来分析）；</p>
<h3 id="1-1-随机变量"><a href="#1-1-随机变量" class="headerlink" title="1.1 随机变量"></a>1.1 随机变量</h3><blockquote>
<p>定义：随机变量X(ω)是一个定义在Ω上的实值单值<code>函数</code>，即该函数的定义域是样本空间Ω（注意其定义域不一定是实数集），值域是实数轴；</p>
</blockquote>
<ul>
<li><p>该函数的自变量ω是样本空间Ω中的基本事件，之所以称该函数为随机变量就是因为自变量ω的出现是有一定的概率，因此其对应的函数值也是随机出现的，需要与普通函数的x是一个确定值区分；</p>
</li>
<li><p>正因为自变量ω不是一个确定的值，因此不能使用微积分工具来研究随机变量函数的性质，于是引出了研究随机变量的工具 – 分布函数；</p>
</li>
</ul>
<p>（一般的随机变量函数X(ω)我们简称为随机变量X，因为之后的所有研究对象都是函数值X所以可以忽略自变量ω）</p>
<h3 id="1-2-分布函数"><a href="#1-2-分布函数" class="headerlink" title="1.2 分布函数"></a>1.2 分布函数</h3><blockquote>
<p>定义：设X是随机变量，x是任意实数，则称函数<code>F(x)=P&#123;X&lt;=x&#125;（x∈R）</code>为随机变量X的分布函数，也称X服从分布F(x)分布，记为X~F(x)</p>
</blockquote>
<p>关于上述定义需要说明：</p>
<ul>
<li>随机变量X的定义域是Ω，但是<code>任何</code>分布函数F(x)的定义域都是R，也就是x属于(-∞，+∞)，在后续解题过程中一定要考虑这点然后分情况处理；</li>
<li>分布函数的x从-∞取值到+∞对应ω从空集取值到Ω；</li>
<li>因为分布函数F(x)中的x是一个确定值，因此可以研究其相关性质；</li>
</ul>
<p>分布函数主要有以下五条基本性质（无论是离散还是连续都有这样的结论，这也是分布函数的<code>充要条件</code>）</p>
<p><img src="/images/image-20230418223318672.png" srcset="/img/loading.gif" lazyload></p>
<p>(4)分布函数本质上是事件的概率，即P{X&lt;&#x3D;x}，因此有0&lt;&#x3D;F(x)&lt;&#x3D;1，即任何分布函数都是有界函数；</p>
<p>(5)离散型随机变量的分布函数是阶梯状的（对应阶梯图像右连续，右连续是指函数f(x)在x<del>0</del>点的取值与limx-&gt;x<del>0</del>^+^f(x)是相同的），连续型随机变量的分布函数是连续的（其概率密度函数f(x)不一定连续）</p>
<p>分布函数一个简单且重要的应用是求概率</p>
<p><img src="/images/image-20230418223720312.png" srcset="/img/loading.gif" lazyload></p>
<blockquote>
<p>注意：</p>
<ul>
<li>关于分布函数右连续这个性质，在解题的过程中非常重要，尤其是在写离散型随机变量的分布函数的时候，区间应该写作[a,b)这样做出的阶梯图像才是右连续的！！！</li>
<li>关于随机变量X和Y同分布(即X和Y都服从相同的分布函数)<ul>
<li>若题干告知X和Y同分布，则表示X和Y的分布函数相同，但并不意味着X和Y的概率密度函数也相同</li>
<li>若题干告知随即变量X&#x3D;Y则两个变量一定同分布且概率密度函数相同</li>
<li>有相同的期望和方差，两个随机变量未必服从相同的分布</li>
</ul>
</li>
</ul>
</blockquote>
<h2 id="2-常见随机变量"><a href="#2-常见随机变量" class="headerlink" title="2.常见随机变量"></a>2.常见随机变量</h2><blockquote>
<ul>
<li><p>分布律是指X取每个值对应的概率，是离散型随机变量特有的；</p>
</li>
<li><p>概率密度函数类似于分布律，同样也是每个X值对应的概率，是连续型随机变量特有的；</p>
</li>
</ul>
</blockquote>
<h3 id="2-1-离散型随机变量"><a href="#2-1-离散型随机变量" class="headerlink" title="2.1 离散型随机变量"></a>2.1 离散型随机变量</h3><blockquote>
<p>定义：如果随机变量X的取值是有限个或可列个值，则称X为离散型随机变量（反之，称X为[连续型随机变量](# 2.2 连续型随机变量)）；</p>
</blockquote>
<p>假如随机变量X是离散型随机变量，则X的分布律、分布列或概率分布如下（即X取每个值的概率）</p>
<p><img src="/images/image-20230418224334681.png" srcset="/img/loading.gif" lazyload></p>
<p>X的分布律常记为X服从p<del>i</del>，一般使用表格或矩阵来表示</p>
<p><img src="/images/image-20230418224438646.png" srcset="/img/loading.gif" lazyload></p>
<p>X的分布函数定义如下</p>
<p><img src="/images/image-20230418224728350.png" srcset="/img/loading.gif" lazyload></p>
<p>离散型随机变量利用分布函数计算其在区间上的概率方式如下</p>
<p><img src="/images/image-20230418225116527.png" srcset="/img/loading.gif" lazyload></p>
<blockquote>
<p>借助分布函数，可以非常精准的求解概率如</p>
<ul>
<li>P{X&lt;&#x3D;a}&#x3D;F(a)</li>
<li>P{X&lt;a}&#x3D;F(a-0)</li>
<li>P{X&#x3D;a}&#x3D;F(a)-F(a-0)</li>
<li>P{a&lt;X&lt;&#x3D;b}&#x3D;F(b)-F(a)</li>
</ul>
<p>上面这四个公式是基础公式，可由它们演化出如下公式（抓住F(x)&#x3D;P{X&lt;&#x3D;x}这一基本定义）</p>
<ul>
<li>P{X&gt;a}&#x3D;1-P{X&lt;&#x3D;a}&#x3D;1-F(a)</li>
<li>P{a&lt;&#x3D;X&lt;&#x3D;b}&#x3D;F(b)-F(a-0)</li>
<li>P{a&lt;X&lt;b}&#x3D;F(b-0)-F(a)</li>
</ul>
</blockquote>
<h3 id="2-2-连续型随机变量"><a href="#2-2-连续型随机变量" class="headerlink" title="2.2 连续型随机变量"></a>2.2 连续型随机变量</h3><blockquote>
<p>定义：如果X的分布函数的形式为<img src="/images/image-20230418224901621.png" srcset="/img/loading.gif" lazyload>，其中f(t)是非负可积的函数，则称X为连续型随机变量，同时称f(t)为X的概率密度函数，记为X~f(t)</p>
</blockquote>
<p>上述定义中，<code>f(x)为某随机变量X的概率密度函数的充要条件是f(x)&gt;=0且f(x)在-∞到+∞上的积分为1</code>；</p>
<p>关于连续型随机变量：</p>
<ul>
<li>连续型随机变量在任意一点的概率都为0；</li>
<li>连续型随机变量的分布函数一定是连续的（但其对应的概率密度函数f(t)不一定是连续的）</li>
</ul>
<p>连续型随机变量利用分布函数计算其在区间上的概率方式如下（连续随机变量的概率计算公式简单得多，因为在点上的概率为0所以无需考虑等号）</p>
<p><img src="/images/image-20230418225157575.png" srcset="/img/loading.gif" lazyload></p>
<h3 id="2-3-概念辨析"><a href="#2-3-概念辨析" class="headerlink" title="2.3 概念辨析"></a>2.3 概念辨析</h3><h4 id="2-3-1-常见问题"><a href="#2-3-1-常见问题" class="headerlink" title="2.3.1 常见问题"></a>2.3.1 常见问题</h4><blockquote>
<p>Q：离散型随机变量嗯好连续型随机变量的本质区别在哪里？</p>
</blockquote>
<p>A：本质区别就在于随机变量的取值是“离散”的还是“连续”的；离散和连续是两个相对的概念，离散是指任取两个数字，其中间只会有有限个数字。不是离散的就一定是连续的（至少现阶段可以这样认为）；</p>
<p>另外需要注意一点，连续并不意味着无法取得其中的某个值，比如对于在连续的实数轴上，我们可以任意取出某个点对应的某个值；</p>
<blockquote>
<p>Q：几何概型和连续型随机变量究竟有没有什么关系？</p>
</blockquote>
<p>A：在考研数学中我们认为这两个概念没有任何关系 – 唯一相似之处就是两者的概念中都会出现“连续”；</p>
<ul>
<li>连续型<code>随机变量的取值</code>是连续的，而<code>几何概型的概率的取值</code>也是连续的，这并不意味着两者之间有什么关系；</li>
</ul>
<p>当然还有人可能会说，几何概型在某点取值的概率与连续型随机变量在某点取值的概率都是0，这难道不是两者之间的联系吗？实际上这两个“点”都不是同一个概念上的点；</p>
<p>-<br>  几何概型在某点取值的概率为0是因为某点的测度为0作为分子因此结果为0；</p>
<ul>
<li>连续型随机变量之所以在某点取值的概率为0是因为在计算概率(分布函数)的公式中会用到积分，而当积分的上下限都是该点的时候自然计算出的结果也是0（至于常说的“测不到”，以后别这么用，这个说法不标准，容易引起混淆 – 因为上述两种情况都可以说是测不到）；</li>
<li>注意一点，硬要给两者攀关系的话，因为连续型随机变量的取值是连续的，这意味着是可以对应几何度量的，因此连续型随机变量在某点的概率取值为0对应了几何度量中的0作为分子(因为点的长度为0)；从这个角度来看连续型随机变量和几何概型还真有联系 – 这也是为什么在这个概念上纠结了一个多小时，因为两者看似没有关系实际上又有关系…而实际上两者的联系也就仅仅如此而已，所以一般为了不混淆，我们索性认为这两个概念根本没有联系，解题的时候用各自的公式求解；</li>
</ul>
<p>从教材上最原始的定义理解，两者也是没有关系的：</p>
<ul>
<li>几何概型的前提是事件必须有几何度量；</li>
<li>连续型随机变量的定义只对事件对应的函数值做了要求，而对事件本身并没有任何限制；</li>
</ul>
<p>综上，在考研概率论中，为了避免不必要的麻烦，我们认为几何概型和连续型随机变量没有任何关系；</p>
<h4 id="2-3-2-易混概念"><a href="#2-3-2-易混概念" class="headerlink" title="2.3.2 易混概念"></a>2.3.2 易混概念</h4><p>从另一种角度来理解离散型随机变量和连续型随机变量</p>
<h5 id="1-概率密度函数-PDF"><a href="#1-概率密度函数-PDF" class="headerlink" title="(1)概率密度函数(PDF)"></a>(1)概率密度函数(PDF)</h5><p><img src="/images/image-20230618203323762.png" srcset="/img/loading.gif" lazyload></p>
<blockquote>
<p>概率密度函数本身不是概率，因此其取值可以大于1；</p>
</blockquote>
<h5 id="2-概率质量函数-PMF"><a href="#2-概率质量函数-PMF" class="headerlink" title="(2)概率质量函数(PMF)"></a>(2)概率质量函数(PMF)</h5><p><img src="/images/image-20230618203529091.png" srcset="/img/loading.gif" lazyload></p>
<blockquote>
<p>概率质量函数本身就代表对应的离散值的概率，因此其取值范围一定在[0,1]；</p>
</blockquote>
<h5 id="3-积累分布函数-CDF"><a href="#3-积累分布函数-CDF" class="headerlink" title="(3)积累分布函数(CDF)"></a>(3)积累分布函数(CDF)</h5><p><img src="/images/image-20230618203717697.png" srcset="/img/loading.gif" lazyload></p>
<blockquote>
<p>积累分布函数的本质也是概率，因此其取值也限定在[0,1]；</p>
</blockquote>
<h2 id="3-八大分布"><a href="#3-八大分布" class="headerlink" title="3.八大分布"></a>3.八大分布</h2><p>概率论八大分布是数理统计中最常用的概率分布(概率分布是指用于表述随机变量取值的概率规律)，它们可以用来研究各种随机现象，并影响着大量实际问题的解决方案，以度量其实际应用的准确性。因此，深入理解概率论八大分布是数理统计和实际应用上的必要，也是重要的一步。（随机变量的分布并不等价于随机变量的分布函数，注意区分！！！）</p>
<p><img src="/images/image-20230428165156675.png" srcset="/img/loading.gif" lazyload alt="常见分布的期望和方差"></p>
<h3 id="3-1-离散型分布"><a href="#3-1-离散型分布" class="headerlink" title="3.1 离散型分布"></a>3.1 离散型分布</h3><h4 id="3-1-1-两点分布-B-1-p"><a href="#3-1-1-两点分布-B-1-p" class="headerlink" title="3.1.1 两点分布_B(1,p)"></a>3.1.1 两点分布_B(1,p)</h4><p>在介绍0-1分布之前先介绍伯努利试验，先从独立试验开始说起；</p>
<ul>
<li>独立试验：如果试验的结果之间相互独立则称试验相互独立；</li>
<li>独立试验序列：在同样条件下独立重复地进行一系列完全相同的试验，即每次试验的可能结果及其发生的概率都不变，每次试验是相互独立的，称这种重复试验序列的数学模型为独立试验序列概型；</li>
<li>伯努利试验：在独立试验的基础上，如果每次试验只有两个结果且互为对立事件，则称该试验为伯努利试验，将伯努利试验独立重复地进行n次称为n重伯努利试验，总结伯努利试验的特点<ul>
<li>只有两种对立的结果；</li>
<li>各次试验相互独立；</li>
<li>各次试验成功的概率相同；</li>
</ul>
</li>
</ul>
<p>0-1分布就是一重伯努利试验，也称为Ber-E&#x2F;两点分布</p>
<blockquote>
<p>定义：</p>
<p><img src="/images/image-20230418231616274.png" srcset="/img/loading.gif" lazyload></p>
</blockquote>
<h4 id="3-1-2-二项分布-B-n-p"><a href="#3-1-2-二项分布-B-n-p" class="headerlink" title="3.1.2 二项分布_B(n,p)"></a>3.1.2 二项分布_B(n,p)</h4><p>二项分布也称为Ber-En分布，上面介绍的两点分布实际上是一种特殊的二项分布，即n&#x3D;1时的二项分布，二项分布的定义如下</p>
<blockquote>
<p>定义：</p>
<p><img src="/images/image-20230418231842611.png" srcset="/img/loading.gif" lazyload></p>
</blockquote>
<ul>
<li>n重伯努利试验服从二项分布，若X是n重伯努利试验中事件A发生的次数，则X~B(n,p)，其中p&#x3D;P(A)；</li>
</ul>
<h4 id="3-1-3-泊松分布-P-λ"><a href="#3-1-3-泊松分布-P-λ" class="headerlink" title="3.1.3 泊松分布_P(λ)"></a>3.1.3 泊松分布_P(λ)</h4><p>泊松分布的理解：某场合下，单位时间内质点来流个数为X(X为自然数)，参数λ称为强度，则随机变量X服从泊松分布P(λ)，且E(X)&#x3D;λ，即单位时间内质点来流的平均个数为λ</p>
<blockquote>
<p>定义：</p>
<p><img src="/images/image-20230419194457147.png" srcset="/img/loading.gif" lazyload></p>
</blockquote>
<ul>
<li>泊松分布的公式是少数无法直接用自然语言转换的、需要强行记忆的公式；</li>
<li>举实例理解泊松分布，某交叉路口的汽车流可看作泊松分布，其中X表示一分钟(单位时间)内汽车通过的数量，假如题目告知一分钟内有汽车通过的概率P{X&gt;&#x3D;1}&#x3D;0.7，则可依据公式求解出强度λ；</li>
</ul>
<h4 id="3-1-4-几何分布-G-p"><a href="#3-1-4-几何分布-G-p" class="headerlink" title="3.1.4 几何分布_G(p)"></a>3.1.4 几何分布_G(p)</h4><p>几何分布也称为Ber-E<del>∞</del>分布，这并不意味着进行无穷次伯努利试验，而是重复进行伯努利试验当首次命中时即停止</p>
<ul>
<li>几何分布与几何没有任何关系（但是几何概型与几何有关系）；</li>
<li>几何分布是一种离散型的等待分布；</li>
</ul>
<blockquote>
<p>定义：</p>
<p><img src="/images/image-20230419194850040.png" srcset="/img/loading.gif" lazyload></p>
</blockquote>
<ul>
<li>设X表示伯努利试验中事件A首次发生所需做的试验次数，则X~G(p)，其中p&#x3D;P(A)</li>
</ul>
<h4 id="3-1-5-超几何分布-H-n-N-M"><a href="#3-1-5-超几何分布-H-n-N-M" class="headerlink" title="3.1.5 超几何分布_H(n,N,M)"></a>3.1.5 超几何分布_H(n,N,M)</h4><blockquote>
<p>定义：</p>
<p><img src="/images/image-20230419195229795.png" srcset="/img/loading.gif" lazyload></p>
</blockquote>
<p>超几何分布常常出现在从一个有限总体中不放回的随机抽样，其中的N表示产品总数，M表示其中的不合格品数量，n表示不放回的随机抽取数量，k表示抽取到的不合格产品的数量（关于k的取值范围实在是没必要记忆，理解为抽取到的不合格产品的数量即可）</p>
<h3 id="3-2-连续型分布"><a href="#3-2-连续型分布" class="headerlink" title="3.2 连续型分布"></a>3.2 连续型分布</h3><p>注意连续型分布的概率密度函数的定义域都统一是开区间，这点需要与分布函数定义域的左闭右开做区分！！！</p>
<h4 id="3-2-1-均匀分布-U-a-b"><a href="#3-2-1-均匀分布-U-a-b" class="headerlink" title="3.2.1 均匀分布_U(a,b)"></a>3.2.1 均匀分布_U(a,b)</h4><p>均匀分布的背景是一维的几何概型，即概率计算方式为测度之比 – 若题目明确说明了随机变量X在区间(a,b)是服从均匀分布的，则在这段区间上的f(x)一定不为0且相等</p>
<blockquote>
<p>定义：</p>
<p><img src="/images/image-20230419195431900.png" srcset="/img/loading.gif" lazyload></p>
<p><img src="/images/image-20230419195754863.png" srcset="/img/loading.gif" lazyload></p>
</blockquote>
<ul>
<li>求解均匀分布的一个重要性质是F((a+b)&#x2F;2)&#x3D;P{X&lt;&#x3D;(a+b)&#x2F;2}&#x3D;1&#x2F;2；</li>
<li>均匀分布的另一个重要性质是其概率密度函数一旦有值则在区间(a,b)上一直有值直到结束；<ul>
<li>换句话说，当x&lt;a时(即起点左边)有F(x)&#x3D;0，x&gt;b时(即起点右边)有F(x)&#x3D;1</li>
</ul>
</li>
</ul>
<h4 id="3-2-2-指数分布-E-λ"><a href="#3-2-2-指数分布-E-λ" class="headerlink" title="3.2.2 指数分布_E(λ)"></a>3.2.2 指数分布_E(λ)</h4><p>指数分布是连续型等待分布（与几何分布区分），计时方式是连续的，其中λ是失效率（失效率与平均等待时间呈倒数关系），有E(X)&#x3D;1&#x2F;λ</p>
<blockquote>
<p>定义：</p>
<p><img src="/images/image-20230419200234277.png" srcset="/img/loading.gif" lazyload></p>
</blockquote>
<ul>
<li>假设某元件的工作寿命X(小时)服从参数为λ的指数分布，则该元件正常工作t小时的概率为P{X&gt;t}&#x3D;1-F(t)；</li>
<li>指数分布具有无记忆性（在无耗损，即失效率不变的情况下），这意味着元件继续正常工作10小时的概率P{X&gt;10}与其之前的工作状态无关，即P{X&gt;10+已工作时长|X已工作时长}&#x3D;P{X&gt;10}；</li>
</ul>
<h4 id="3-2-3-正态分布-N-μ-σ-2"><a href="#3-2-3-正态分布-N-μ-σ-2" class="headerlink" title="3.2.3 正态分布_N(μ,σ^2^)"></a>3.2.3 正态分布_N(μ,σ^2^)</h4><p>正态分布是最重要的分布，在后面会介绍，所有的分布最终都会趋于正态分布</p>
<blockquote>
<p>定义：</p>
<p><img src="/images/image-20230419200930978.png" srcset="/img/loading.gif" lazyload></p>
</blockquote>
<ul>
<li>正态分布曲线图，均值不变，方差越大，图像越低，随机变量的取值越分散（因为方差的本质就是表示随机变量取值的分散程度）</li>
</ul>
<p>比正态分布更重要的是标准正态分布</p>
<blockquote>
<p>定义：</p>
<p><img src="/images/image-20230419201232816.png" srcset="/img/loading.gif" lazyload></p>
</blockquote>
<p>关于正态分布主要有以下结论：</p>
<p><img src="/images/image-20230419201355466.png" srcset="/img/loading.gif" lazyload></p>
<h2 id="4-一维随机变量函数的分布"><a href="#4-一维随机变量函数的分布" class="headerlink" title="4.一维随机变量函数的分布*"></a>4.一维随机变量函数的分布*</h2><p>设X为随机变量，有函数y&#x3D;g(x)，称Y&#x3D;g(X)函数为以随机变量X为自变量的函数，该函数仍然是随机变量；</p>
<p>随机变量的函数仍然是随机变量，所以仍然可以研究其分布，随机变量函数的分布主要分为两类：</p>
<ul>
<li><code>自变量为离散型-&gt;因变量为离散型</code><ul>
<li>这种形式较为简单，直接对应概率相加即可；</li>
</ul>
</li>
</ul>
<p><img src="/images/image-20230419202352253.png" srcset="/img/loading.gif" lazyload></p>
<ul>
<li><code>自变量为连续型-&gt;因变量为混合型/连续型</code>（此处的混合指的是，因变量在区间端点离散取值，在区间内部连续取值，即同时具有连续型和离散型随机变量特征，其分布函数的图像大致如<img src="/images/image-20230710204540320.png" srcset="/img/loading.gif" lazyload>)<ul>
<li>这种形式用分布函数法解决，即先求分布函数F<del>Y</del>(y)再求f<del>Y</del>(y)</li>
</ul>
</li>
</ul>
<p><img src="/images/image-20230419202333110.png" srcset="/img/loading.gif" lazyload></p>
<p>对于上述因变量为混合型&#x2F;连续型的问题，更一般的步骤为：</p>
<ol>
<li>明确Y&#x3D;F(X)是什么，设Y的分布函数为G<del>Y</del>(y)，概率密度函数为g<del>Y</del>(y)；</li>
<li>因为y的范围同样也是(-∞,+∞)，因此分情况讨论在不同情况下的G<del>Y</del>(y)&#x3D;P{Y&lt;&#x3D;y}&#x3D;P{F(X)&lt;&#x3D;y}；</li>
<li>根据g<del>Y</del>(y)&#x3D;G<del>Y</del>(y)’求解出对应的概率密度函数；</li>
</ol>
<h1 id="第三讲-多维随机变量及其分布"><a href="#第三讲-多维随机变量及其分布" class="headerlink" title="第三讲 多维随机变量及其分布"></a>第三讲 多维随机变量及其分布</h1><p>显然一维随机变量只能表示单个特征，为了表示多维特征，因此需要引入多维随机变量，一般以研究二维随机变量为主。</p>
<h2 id="1-基本概念-2"><a href="#1-基本概念-2" class="headerlink" title="1.基本概念"></a>1.基本概念</h2><p>一维随机变量的定义：随机变量X(ω)是一个定义在Ω上的实值单值函数，即该函数的定义域是样本空间Ω（注意其定义域不一定是实数集），值域是实数轴；</p>
<blockquote>
<p>n维随机变量的定义：若X<del>1</del>,X<del>2</del>…X<del>n</del>是定义在<code>同一个样本空间Ω上</code>的n个一维随机变量，则称(X<del>1</del>,X<del>2</del>…X<del>n</del>)为n维随机变量或n维随机向量；</p>
</blockquote>
<ul>
<li>一般研究n维随机变量以二维随机变量为主。无论是连续型还是离散型，二维随机变量的分布函数主要分为三类：联合分布函数、边缘分布函数和条件分布函数</li>
</ul>
<h3 id="1-1-联合分布函数"><a href="#1-1-联合分布函数" class="headerlink" title="1.1 联合分布函数"></a>1.1 联合分布函数</h3><blockquote>
<p>定义：记为(X<del>1</del>,X<del>2</del>…X<del>n</del>)服从F(x<del>1</del>,x<del>2</del>…x<del>n</del>)</p>
<p><img src="/images/image-20230420224739341.png" srcset="/img/loading.gif" lazyload></p>
</blockquote>
<ul>
<li>对于二维随机变量(X,Y)来说，其联合分布函数为F(x,y)，F(x,y)是事件A&#x3D;{X&lt;&#x3D;x}与B&#x3D;{Y&lt;&#x3D;y}同时发生的概率；</li>
</ul>
<p>二维随机变量的联合分布函数主要有如下性质</p>
<p><img src="/images/image-20230420225511203.png" srcset="/img/loading.gif" lazyload></p>
<h3 id="1-2-边缘分布函数"><a href="#1-2-边缘分布函数" class="headerlink" title="1.2 边缘分布函数"></a>1.2 边缘分布函数</h3><p>二维随机变量(X,Y)的联合分布函数为F(x,y)，则随机变量X与Y的分布函数F<del>X</del>(x)与F<del>Y</del>(y)分别称为(X,Y)关于X和关于Y的边缘分布函数；</p>
<p>F<del>X</del>(x)和F<del>Y</del>(y)的计算方式非常简单，由概率性质可得（图中的笔记是如何利用联合分布函数和边缘分布函数判断X和Y两个随机变量独立，是边缘分布的一个应用，在[随机变量的独立性](# 4.随机变量的独立性)会有更详细的介绍）</p>
<p><img src="/images/image-20230420230031860.png" srcset="/img/loading.gif" lazyload></p>
<p><img src="/images/image-20230420230053123.png" srcset="/img/loading.gif" lazyload></p>
<h2 id="2-常见二维随机变量"><a href="#2-常见二维随机变量" class="headerlink" title="2.常见二维随机变量"></a>2.常见二维随机变量</h2><p>这里给出离散型和连续型随机变量的概念汇总，这也是解题需要掌握的内容（本章的核心）</p>
<p><img src="/images/image-20230710203508894.png" srcset="/img/loading.gif" lazyload></p>
<blockquote>
<ul>
<li><p>混合型的二维随机变量会在多维随机变量函数的分布那一节的知识点中考察，也就是X和Y中一个是离散型一个是连续型，使用分布函数搭配全集分解思想求解即可；</p>
</li>
<li><p>分布律是指X取每个值对应的概率，是离散型随机变量特有的；</p>
</li>
<li><p>概率密度函数类似于分布律，同样也是每个X值对应的概率，是连续型随机变量特有的；</p>
</li>
</ul>
</blockquote>
<h3 id="2-1-离散型随机变量-1"><a href="#2-1-离散型随机变量-1" class="headerlink" title="2.1 离散型随机变量"></a>2.1 离散型随机变量</h3><blockquote>
<p>二维离散型随机变量定义：如果二维随机变量(X,Y)只能取有限对值或可列对值，则称(X,Y)为二维离散型随机变量</p>
</blockquote>
<p>离散型<code>联合分布律</code>以及<code>边缘分布律</code>定义如下</p>
<p><img src="/images/image-20230420230537648.png" srcset="/img/loading.gif" lazyload></p>
<p>二维离散型变量的分布函数主要分为三类：联合分布函数、边缘分布函数和条件分布函数（当然下面展示的其实是联合分布函数、边缘分布律和条件分布律 – 分布函数不等于分布律，一定要注意这一点）</p>
<p><code>(X,Y)</code>的<code>联合分布函数</code>的定义如下</p>
<p><img src="/images/image-20230420230717678.png" srcset="/img/loading.gif" lazyload></p>
<p><code>X,Y</code>的<code>边缘分布律</code>分别为</p>
<p><img src="/images/image-20230420230903806.png" srcset="/img/loading.gif" lazyload></p>
<p>二维离散型随机变量的<code>条件分布律</code>的定义如下</p>
<p><img src="/images/image-20230420231017491.png" srcset="/img/loading.gif" lazyload></p>
<h3 id="2-2-连续型随机变量-1"><a href="#2-2-连续型随机变量-1" class="headerlink" title="2.2 连续型随机变量"></a>2.2 连续型随机变量</h3><blockquote>
<p>二维连续型随机变量定义：</p>
<p><img src="/images/image-20230420231149028.png" srcset="/img/loading.gif" lazyload></p>
</blockquote>
<p>若(X,Y)的联合分布函数为F(x,y)，联合概率密度函数为f(x,y)，则有如下性质成立（<code>其中最重要的是2，3</code>）</p>
<p><img src="/images/image-20230621195740225.png" srcset="/img/loading.gif" lazyload></p>
<blockquote>
<p>上述混合求偏导的时候，对谁先求偏导没有要求（结果都是一样的），注意x和y是两个没有关系的变量因此不会有链式法则的出现；</p>
</blockquote>
<p>若(X,Y)~f(x,y)，则X的边缘分布函数和边缘密度函数分别为（Y的边缘分布函数和边缘密度函数同理）</p>
<p><img src="/images/image-20230420231756658.png" srcset="/img/loading.gif" lazyload></p>
<blockquote>
<p>边缘概率密度函数的计算如果直接从高数的思维来理解很容易绕晕，这里的口诀就是用于快速计算边缘概率密度的，不要把它和高数中的二重积分或者一重积分的几何运用搞混；</p>
<ul>
<li>其中“不积先定限”指的不是上下限，而是<code>范围限</code>（这就是和二重积分的区别），尽管该一重积分形式上是负无穷到正无穷，但实际上只有在区域内的被积函数不为0，其余为0的范围就算再大算出来的面积也只能是0无意义；</li>
<li>边缘密度函数的图像仅仅只是为了确定有意义的范围限，并要求去使用一重积分计算几何区域的面积，所以使用一重积分的思想就是无稽之谈；</li>
</ul>
</blockquote>
<p>若(X,Y)~f(x,y)，则条件概率密度函数的定义如下</p>
<p><img src="/images/image-20230420231922936.png" srcset="/img/loading.gif" lazyload></p>
<p>类比联合概率密度函数和联合分布函数的关系，可以得到二维连续型随机变量的条件分布函数的定义</p>
<p><img src="/images/image-20230420232026021.png" srcset="/img/loading.gif" lazyload></p>
<h3 id="2-3-小结"><a href="#2-3-小结" class="headerlink" title="2.3 小结"></a>2.3 小结</h3><ul>
<li>已知(X,Y)的联合分布（包括联合分布函数F(x,y)，联合概率密度函数f(x,y)，以及联合分布律p<del>ij</del>），则可以确定边缘分布（包括边缘分布函数、边缘概率密度函数以及边缘分布律）和条件分布（包括条件分布函数、条件分布律或条件概率密度函数）；</li>
<li>反之，仅知道边缘分布无法确定其联合分布（除非附加其他条件）；</li>
</ul>
<p>(1)离散型随机变量的联合分布与边缘分布、条件分布之间的转换</p>
<p><img src="/images/image-20230424145023577.png" srcset="/img/loading.gif" lazyload></p>
<p>（2）连续型随机变量的联合分布与边缘分布、条件分布之间的转换</p>
<p><img src="/images/image-20230424145109497.png" srcset="/img/loading.gif" lazyload></p>
<h2 id="3-常见二维分布"><a href="#3-常见二维分布" class="headerlink" title="3.常见二维分布"></a>3.常见二维分布</h2><h3 id="3-1-二维均匀分布"><a href="#3-1-二维均匀分布" class="headerlink" title="3.1 二维均匀分布"></a>3.1 二维均匀分布</h3><p>二维均匀分布的背景是二维几何概型（即“操场掉馅饼，用碗接”）</p>
<blockquote>
<p>定义：</p>
<p><img src="/images/image-20230420232940660.png" srcset="/img/loading.gif" lazyload></p>
</blockquote>
<ul>
<li>关于二维均匀分布，很多人会在f(x,y)的区间为开区间还是闭区间上纠结，实际上开区间和闭区间都是正确的，但是习惯上都写作开区间；</li>
</ul>
<h3 id="3-2-二维正态分布"><a href="#3-2-二维正态分布" class="headerlink" title="3.2 二维正态分布"></a>3.2 二维正态分布</h3><p><img src="/images/image-20230621210402489.png" srcset="/img/loading.gif" lazyload></p>
<h2 id="4-随机变量的独立性"><a href="#4-随机变量的独立性" class="headerlink" title="4.随机变量的独立性"></a>4.随机变量的独立性</h2><p>之所以在一维随机变量中不讨论独立性是因为一维的随机变量只有一个不存在与谁独立，但是在二维以及多维随机变量中需要研究该问题，但是也非常简单，前面[事件的独立性](# 6.事件的独立性)中介绍了两个事件相互独立需要满足的条件，因为随机变量是事件的量化表示，所以随机变量的独立性定义与之类似。</p>
<blockquote>
<p>定义：</p>
<p><img src="/images/image-20230424145828807.png" srcset="/img/loading.gif" lazyload></p>
<p><img src="/images/image-20230424150216521.png" srcset="/img/loading.gif" lazyload></p>
</blockquote>
<p>简单概括来说，对于离散型随机变量，其相互独立的充要条件为（更常用的判断独立的方式，即使用分布律或概率密度函数）</p>
<p><img src="/images/image-20230424150436907.png" srcset="/img/loading.gif" lazyload></p>
<p>对于连续型随机变量，其相互独立的充要条件为</p>
<p><img src="/images/image-20230424150558407.png" srcset="/img/loading.gif" lazyload></p>
<p>若多维随机变量相互独立，则具有如下性质（其中第三条最重要！！！）</p>
<p><img src="/images/image-20230424150702725.png" srcset="/img/loading.gif" lazyload></p>
<p><img src="/images/image-20230424150908081.png" srcset="/img/loading.gif" lazyload></p>
<blockquote>
<p>“设(X,Y)是二维随机变量，若X与Y相互独立，则条件分布等于边缘分布。”这条性质的逆用也非常实用，拿连续型随机变量举例，若X和Y相互独立，则联合概率密度f(x,y)等于边缘密度的乘积f<del>X</del>(x)·f<del>Y</del>(y)。</p>
</blockquote>
<h2 id="5-多维随机变量函数的分布"><a href="#5-多维随机变量函数的分布" class="headerlink" title="5.多维随机变量函数的分布"></a>5.多维随机变量函数的分布</h2><p>已知X,Y为随机变量，g(x,y)是二元函数，那么以随机变量X,Y作为自变量的函数U&#x3D;g(X,Y)也是随机变量，被称为随机变量X,Y的函数。</p>
<p>本节求解的问题为：</p>
<ul>
<li>已知(X,Y)的联合分布，求U&#x3D;g(X,Y)的分布；</li>
<li>已知(X,Y)的联合分布，且U&#x3D;g(X,Y)，V&#x3D;h(X,Y)，求(U,V)的分布；</li>
</ul>
<p>主要可分为如下三种类型的题型（当然前两种类型最常见）：</p>
<ul>
<li>(离散,离散)-&gt;离散：先确定函数Z的值后，求其相应的概率，一般使用矩阵法；</li>
<li>(连续,连续)-&gt;连续：<ul>
<li>分布函数法：<img src="/images/image-20230424152343607.png" srcset="/img/loading.gif" lazyload></li>
<li>卷积公式法（分布函数好理解而且万能，卷积公式针对某些特定题型可以较快解决，个人推荐在熟练掌握分布函数法的基础上再尝试使用卷积公式法）</li>
</ul>
</li>
<li>(离散,连续)-&gt;连续：使用“全集分解思想”，即将事件对离散型的一切可能值进行全集分解，然后利用全概率公式求解；</li>
</ul>
<p>对于上述第三种形式，给出下面这道例题帮助理解（这道题具备一般性，很有意义）</p>
<p><img src="/images/image-20230427172150629.png" srcset="/img/loading.gif" lazyload></p>
<h3 id="5-1-离散-离散-gt-离散"><a href="#5-1-离散-离散-gt-离散" class="headerlink" title="5.1 (离散,离散)-&gt;离散"></a>5.1 (离散,离散)-&gt;离散</h3><p>矩阵法解题的大致方式如下</p>
<p><img src="/images/image-20230424153455946.png" srcset="/img/loading.gif" lazyload></p>
<h3 id="5-2-连续-连续-gt-连续"><a href="#5-2-连续-连续-gt-连续" class="headerlink" title="5.2 (连续,连续)-&gt;连续"></a>5.2 (连续,连续)-&gt;连续</h3><p>二维随机变量函数的分布函数法如下</p>
<p><img src="/images/image-20230424153903425.png" srcset="/img/loading.gif" lazyload alt="分布函数法"></p>
<p>对上式进行恒等变形就可以得到卷积公式 – “积谁不换谁，换完求偏导”（注意上面是先dx再dy，下面是先dy再dx，但是对应的有效积分区域都是一样的）</p>
<p><img src="/images/image-20230424154217962.png" srcset="/img/loading.gif" lazyload></p>
<p>进一步的，若随机变量X与Y相互独立，还可以将卷积公式写为如下形式(并不是说X的概率密度函数的参数只能是x，同理Y的概率密度函数也可以是除了y的其他，因为说白了x和y都只是代表数值的变量而已)</p>
<p><img src="/images/image-20230424154302943.png" srcset="/img/loading.gif" lazyload></p>
<p>当然并不只是简单的将该公式写出来就行了，还需要借助口诀“不积先定限，限内画条线，先交为下限，后交为上限”将在(-∞,+∞）的积分公式求解出来，主要借助正概率密度区间。</p>
<h4 id="5-2-1-常见卷积公式"><a href="#5-2-1-常见卷积公式" class="headerlink" title="5.2.1 常见卷积公式"></a>5.2.1 常见卷积公式</h4><p>下面展示常见的卷积公式</p>
<p><img src="/images/image-20230424154438993.png" srcset="/img/loading.gif" lazyload></p>
<p><img src="/images/image-20230424154450803.png" srcset="/img/loading.gif" lazyload></p>
<p><img src="/images/image-20230424154519056.png" srcset="/img/loading.gif" lazyload></p>
<p><img src="/images/image-20230424154552933.png" srcset="/img/loading.gif" lazyload></p>
<p>max函数和min函数并不是用卷积公式求解的，下面的求解方法具有代表性和一般性，值得学习</p>
<p><img src="/images/image-20230424154610445.png" srcset="/img/loading.gif" lazyload></p>
<h4 id="5-2-2-补充知识点"><a href="#5-2-2-补充知识点" class="headerlink" title="5.2.2 补充知识点"></a>5.2.2 补充知识点</h4><p>一些特殊的独立同分布的随机变量，其和的分布也是同类型的，分别如下</p>
<p><img src="/images/image-20230424154832345.png" srcset="/img/loading.gif" lazyload></p>
<p>如果随机变量X和Y均服从正态分布(注意这里并不要求一定独立，两个变量之间的相关性由协方差确定)，则有以下结论（关于第四点有争议，参考意义不大）</p>
<p><img src="/images/image-20230427170206964.png" srcset="/img/loading.gif" lazyload></p>
<h1 id="第四讲-随机变量的数字特征"><a href="#第四讲-随机变量的数字特征" class="headerlink" title="第四讲 随机变量的数字特征"></a>第四讲 随机变量的数字特征</h1><p>在大多数时候时候并不会直接将整个随机变量的分布告知，此时若需要研究随机变量，需要寻找其数字特征。如下是第四讲的核心知识框架</p>
<p><img src="/images/image-20230807150759609.png" srcset="/img/loading.gif" lazyload></p>
<h2 id="1-一维随机变量的数字特征"><a href="#1-一维随机变量的数字特征" class="headerlink" title="1.一维随机变量的数字特征"></a>1.一维随机变量的数字特征</h2><h3 id="1-1-数学期望"><a href="#1-1-数学期望" class="headerlink" title="1.1 数学期望"></a>1.1 数学期望</h3><h4 id="1-1-1-数学期望的定义"><a href="#1-1-1-数学期望的定义" class="headerlink" title="1.1.1 数学期望的定义"></a>1.1.1 数学期望的定义</h4><blockquote>
<p>离散型随机变量及其函数的数学期望定义：</p>
<p><img src="/images/image-20230428162545587.png" srcset="/img/loading.gif" lazyload></p>
</blockquote>
<blockquote>
<p>连续型随机变量及其函数的数学期望定义：</p>
<p><img src="/images/image-20230428162747192.png" srcset="/img/loading.gif" lazyload></p>
</blockquote>
<ul>
<li>根据上述定义可知，一维随机变量有概率密度或分布律不一定表示其一定有期望；</li>
<li>连续型随机变量的函数的数学期望的公式非常好理解，把x那一点的取值用g(x)替代即可，其取值对应的概率密度f(x)不变；</li>
<li><code>数学期望又被称为概率平均值，常常简称为期望或均值</code>，主要刻画了随机变量的一切可能值的集中位置；</li>
</ul>
<h4 id="1-1-2-数学期望的性质"><a href="#1-1-2-数学期望的性质" class="headerlink" title="1.1.2 数学期望的性质"></a>1.1.2 数学期望的性质</h4><p>对于数学期望，主要有以下性质：</p>
<p><img src="/images/image-20230428163903538.png" srcset="/img/loading.gif" lazyload></p>
<ul>
<li>随机变量的数学期望是一个常数，常数的期望是其本身；</li>
</ul>
<h3 id="1-2-方差-amp-标准差"><a href="#1-2-方差-amp-标准差" class="headerlink" title="1.2 方差&amp;标准差"></a>1.2 方差&amp;标准差</h3><h4 id="1-2-1-方差的定义"><a href="#1-2-1-方差的定义" class="headerlink" title="1.2.1 方差的定义"></a>1.2.1 方差的定义</h4><blockquote>
<p>随机变量方差的定义：</p>
<p><img src="/images/image-20230428164038615.png" srcset="/img/loading.gif" lazyload></p>
</blockquote>
<h4 id="1-2-2-方差的性质"><a href="#1-2-2-方差的性质" class="headerlink" title="1.2.2 方差的性质"></a>1.2.2 方差的性质</h4><p>随机变量的方差通常有如下性质：</p>
<p><img src="/images/image-20230428164249210.png" srcset="/img/loading.gif" lazyload></p>
<h3 id="1-3-切比雪夫不等式"><a href="#1-3-切比雪夫不等式" class="headerlink" title="1.3 切比雪夫不等式"></a>1.3 切比雪夫不等式</h3><p>切比雪夫不等式主要描述了这样一件事：随机变量X与其数学期望EX的差值不会很大，即该差值&gt;&#x3D;ε的概率会很小</p>
<blockquote>
<p>定义：</p>
<p><img src="/images/image-20230428164730058.png" srcset="/img/loading.gif" lazyload></p>
</blockquote>
<ul>
<li>由切比雪夫不等式知，当DX愈小时，概率P{|X-EX|&lt;ξ}愈大(即X和EX比较接近，随机变量的波动性较小)，这表明方差是刻画随机变量与其期望值偏离程度的量，是描述随机变量X“分散程度”特征的指标；</li>
<li>使用切比雪夫不等式估算随机变量在某取值的范围内的概率，比如估计概率P{|X-Y|&gt;&#x3D;6}，则令Z&#x3D;X-Y(绝对值内部的表达式)，取ξ&#x3D;6即可证明；</li>
</ul>
<h2 id="2-二维随机变量的数字特征"><a href="#2-二维随机变量的数字特征" class="headerlink" title="2.二维随机变量的数字特征"></a>2.二维随机变量的数字特征</h2><h3 id="2-1-数学期望"><a href="#2-1-数学期望" class="headerlink" title="2.1 数学期望"></a>2.1 数学期望</h3><p>设X,Y为随机变量，g(X,Y)为X,Y的函数，其中g是连续函数</p>
<blockquote>
<p>二维离散型随机变量函数的数学期望：</p>
<p><img src="/images/image-20230428165607351.png" srcset="/img/loading.gif" lazyload></p>
</blockquote>
<blockquote>
<p>二维连续型随机变量函数的数学期望：</p>
<p><img src="/images/image-20230428165703822.png" srcset="/img/loading.gif" lazyload></p>
</blockquote>
<hr>
<blockquote>
<p>Q：为什么只有二维随机变量的函数的期望而没有二维随机变量的期望？</p>
</blockquote>
<p>A：其实关于二维随机变量的期望和方差是有定义的，但是一般我们不会考察，下面给出二维随机变量的期望和方差定义（因为两个变量之间的关系不确定因此使用空间矩阵表示）</p>
<p><img src="/images/image-20230712163303376.png" srcset="/img/loading.gif" lazyload></p>
<hr>
<h3 id="2-2-协方差与相关系数"><a href="#2-2-协方差与相关系数" class="headerlink" title="2.2 协方差与相关系数"></a>2.2 协方差与相关系数</h3><h4 id="2-2-1-定义"><a href="#2-2-1-定义" class="headerlink" title="2.2.1 定义"></a>2.2.1 定义</h4><blockquote>
<p>协方差的定义：</p>
<p><img src="/images/image-20230428165901170.png" srcset="/img/loading.gif" lazyload></p>
</blockquote>
<blockquote>
<p>相关系数的定义：</p>
<p><img src="/images/image-20230428165929531.png" srcset="/img/loading.gif" lazyload></p>
</blockquote>
<ul>
<li>协方差只表示线性相关的方向，取值范围为(-∞,+∞)<ul>
<li>协方差为正值，说明一个变量变大另一个变量也变大；取负值说明一个变量变大另一个变量变小，取0说明两个变量没有相关关系</li>
</ul>
</li>
<li>相关系数不仅表示线性相关的方向，还表示线性相关的程度，取值范围为[-1,+1]<ul>
<li>相关系数为正值，说明一个变量变大另一个变量也变大；取负值说明一个变量变大另一个变量变小，取0说明两个变量没有相关关系。同时，相关系数的绝对值越接近1，线性关系越显著</li>
</ul>
</li>
<li>相关系数是标准化后的特殊协方差，相关系数比协方差多阐述了线性相关的程度。相关系数等于0表示X与Y之间不存在线性关系，称为不相关，但X和Y之间可能存在其它非线性关系；</li>
</ul>
<h4 id="2-2-2-性质"><a href="#2-2-2-性质" class="headerlink" title="2.2.2 性质"></a>2.2.2 性质</h4><p>协方差和相关系数主要有如下性质：</p>
<p><img src="/images/image-20230428170620758.png" srcset="/img/loading.gif" lazyload></p>
<h2 id="3-独立性与相关性"><a href="#3-独立性与相关性" class="headerlink" title="3.独立性与相关性"></a>3.独立性与相关性</h2><h3 id="3-1-独立性"><a href="#3-1-独立性" class="headerlink" title="3.1 独立性"></a>3.1 独立性</h3><p>通过分布来判断独立性（独立就是指没有任何关系）</p>
<p><img src="/images/image-20230428171719769.png" srcset="/img/loading.gif" lazyload></p>
<h3 id="3-2-相关性"><a href="#3-2-相关性" class="headerlink" title="3.2 相关性"></a>3.2 相关性</h3><p>通过数字特征来判断相关性</p>
<p><img src="/images/image-20230428171801000.png" srcset="/img/loading.gif" lazyload></p>
<p>相关性和独立性有以下几个重要结论：</p>
<ul>
<li><p>如果(X,Y)服从二维正态分布，则X和Y相互独立；</p>
</li>
<li><p>如果X与Y相互独立，则X,Y不相关，反之不一定；</p>
</li>
<li><p>如果X与Y相关，则X,Y不独立；</p>
</li>
</ul>
<p>一般解题，要求讨论随机变量X和Y之间的相关性和独立性时，先计算协方差判断相关性，再通过分布判断独立性</p>
<p><img src="/images/image-20230428172258208.png" srcset="/img/loading.gif" lazyload></p>
<h1 id="第五讲-大数定律与中心极限定理"><a href="#第五讲-大数定律与中心极限定理" class="headerlink" title="第五讲 大数定律与中心极限定理"></a>第五讲 大数定律与中心极限定理</h1><p>第五讲作为概率论知识点中的最后一讲，其主要意义在于将概率论中的知识点进行汇总并适当与数理统计联系作为章节过渡。</p>
<p>大数定律用最简单的话来说就是“测试的次数越多得到的实验结论越准确”；中心极限定理的“中心”指的是这个系列定理很重要，用最简单的话来说就是“任何大量相互独立的随机变量的和的极限分布是正态分布”。</p>
<p>为什么我要用最简单的话来描述大数定律和中心极限定理？因为本讲的内容重点在于理解而不是记忆其形式，所以最重要也是最核心的就是记住每个定律或定理到底说了什么。</p>
<h2 id="1-依概率收敛"><a href="#1-依概率收敛" class="headerlink" title="1.依概率收敛"></a>1.依概率收敛</h2><blockquote>
<p>设随机变量X与随机变量序列{X<del>n</del>}（其中n&#x3D;1,2,3…），如果对任意的ε&gt;0有</p>
<p><img src="/images/image-20230515110634228.png" srcset="/img/loading.gif" lazyload></p>
<p>则称随机变量序列{X<del>n</del>}依概率收敛于随机变量X，记作</p>
<p><img src="/images/image-20230515110918640.png" srcset="/img/loading.gif" lazyload></p>
</blockquote>
<p>这里简单解释一下定义中的变量含义：</p>
<ul>
<li>X是随机变量，代表了所有事件的集合（即事件可能发生的所有结果）；</li>
<li>X<del>n</del>表示试验n次事件发生的集合；</li>
<li>依概率收敛说的就是，当n趋于无穷也就是当试验次数足够大的时候，事件发生的集合X<del>n</del>几乎和事件可能发生的结果的集合X一致；</li>
</ul>
<p>公式中的|X<del>n</del>-X|&lt;ε表示的是一个事件，其概率衡量了两个集合的相似程度，此处的随机变量X也可以用常数a来替代；</p>
<h2 id="2-大数定律"><a href="#2-大数定律" class="headerlink" title="2.大数定律"></a>2.大数定律</h2><p>大数定律告诉我们可以用频率近似代替概率，能用样本均值近似代替总体均值(数学期望)。大数定理将属于数理统计的样本均值和属于概率论的数学期望(总体均值)联系在一起，为用统计方法来估计期望提供了理论依据。（大数定律的考点主要在其使用条件，而n-&gt;∞作为默认使用条件也应当注意）</p>
<table>
<thead>
<tr>
<th>大数定律</th>
<th>分布</th>
<th>期望</th>
<th>方差</th>
<th>用途</th>
</tr>
</thead>
<tbody><tr>
<td>伯努利</td>
<td>二项分布</td>
<td>相同</td>
<td>相同</td>
<td>估算概率</td>
</tr>
<tr>
<td>辛钦</td>
<td>独立同分布</td>
<td>相同</td>
<td>相同</td>
<td>估算期望</td>
</tr>
<tr>
<td>切比雪夫</td>
<td>独立</td>
<td>存在</td>
<td>存在且有上界</td>
<td>估算期望</td>
</tr>
</tbody></table>
<h3 id="2-1-切比雪夫大数定律"><a href="#2-1-切比雪夫大数定律" class="headerlink" title="2.1 切比雪夫大数定律*"></a>2.1 切比雪夫大数定律*</h3><p>速记：样本均值依概率收敛到期望的均值(等价于均值的期望，也就是数学期望)，是所有大数定理的本质</p>
<p><img src="/images/image-20230515121514408.png" srcset="/img/loading.gif" lazyload></p>
<ul>
<li>相比于辛钦大数定律，切比雪夫并没有要求随机变量是同分布的，因此更具一般性；</li>
<li>切比雪夫大数定律的使用条件“为{X<del>n</del>}相互独立和DX<del>i</del>存在且一致有上界C”，出题点常在计算DX<del>i</del>得出其无上界，进而不服从切比雪夫大数定律</li>
</ul>
<h3 id="2-2-伯努利大数定律"><a href="#2-2-伯努利大数定律" class="headerlink" title="2.2 伯努利大数定律"></a>2.2 伯努利大数定律</h3><p>速记：频率依概率收敛到概率，由切比雪夫不等式推导</p>
<p><img src="/images/image-20230515121609362.png" srcset="/img/loading.gif" lazyload></p>
<h3 id="2-3-辛钦大数定律"><a href="#2-3-辛钦大数定律" class="headerlink" title="2.3 辛钦大数定律"></a>2.3 辛钦大数定律</h3><p>速记：样本均值依概率收敛到数学期望，是伯努利大数定理的推广</p>
<p><img src="/images/image-20230515121713967.png" srcset="/img/loading.gif" lazyload></p>
<ul>
<li>因为{X<del>n</del>}是独立同分布的随机变量序列，可以推出X<del>i</del>的期望都相同即E(X<del>i</del>)&#x3D;u；</li>
<li>当X<del>i</del>是服从0-1分布的随机变量时，辛钦大数定律就是伯努利大数定律，换句话说伯努利大数定律是辛钦大数定律的一种特例情况；</li>
<li>辛钦大数定律的使用条件为“{X<del>n</del>}独立同分布且EX<del>i</del>存在”，出题点常在“EX<del>i</del>存在”，比如“{X<del>n</del>}服从同一离散型分布”或“{X<del>n</del>}服从同一连续型分布”都不能保证数学期望一定存在，因此不服从辛钦大数定律；</li>
</ul>
<h2 id="3-中心极限定理"><a href="#3-中心极限定理" class="headerlink" title="3.中心极限定理"></a>3.中心极限定理</h2><p>中心极限定理用一句话概括：“在一定条件下，大量独立同分布的随机变量的和的极限分布是正态分布”。</p>
<h3 id="3-1-列维-林德伯格定理"><a href="#3-1-列维-林德伯格定理" class="headerlink" title="3.1 列维-林德伯格定理*"></a>3.1 列维-林德伯格定理*</h3><p><img src="/images/image-20230807154226435.png" srcset="/img/loading.gif" lazyload alt="独立同分布中心极限定理"></p>
<blockquote>
<ul>
<li>定理的三个条件“独立、同分布、期望和方差存在”缺一不可（注意具有相同的期望和方差并不一定同分布）；</li>
<li>该定理说明了这样一件事，当独立同分布的随机变量序列{X<del>n</del>}满足EX<del>i</del>&#x3D;u,DX<del>i</del>&#x3D;σ^2^时，有求ΣX~i~~(nu,nσ^2^)</li>
<li>该定理有另一种变式，即</li>
</ul>
<p><img src="/images/image-20230807154406310.png" srcset="/img/loading.gif" lazyload></p>
</blockquote>
<h3 id="3-2-棣莫弗-拉普拉斯定理"><a href="#3-2-棣莫弗-拉普拉斯定理" class="headerlink" title="3.2 棣莫弗-拉普拉斯定理"></a>3.2 棣莫弗-拉普拉斯定理</h3><p><img src="/images/image-20230807154949962.png" srcset="/img/loading.gif" lazyload alt="二项分布中心极限定理"></p>
<blockquote>
<ul>
<li>拉普拉斯定理是由林德伯格定理推导得来的，因为二项分布可以认为是由n个两点分布求和得到</li>
</ul>
</blockquote>
<h1 id="第六讲-数理统计"><a href="#第六讲-数理统计" class="headerlink" title="第六讲 数理统计"></a>第六讲 数理统计</h1><h2 id="1-总体与样本"><a href="#1-总体与样本" class="headerlink" title="1.总体与样本"></a>1.总体与样本</h2><p>前面已经讲过数理统计和概率论的区别，数理统计是一门基于现实的，根据观察到的结果来估计未知概率规律的学科。</p>
<p>下面介绍几个数理统计中最基础的概念：</p>
<ul>
<li>总体：研究的对象的全体称为总体(工厂生产的所有灯泡)，组成总体的每一个元素称为个体(每一个灯泡)。数理统计中的总体等同于概率论中的随机变量X的概念，故总体的分布实际就是指随机变量X的分布；</li>
<li>样本&amp;样本值：n个独立同分布于总体的随机变量X<del>1</del>,X<del>2</del>…X<del>n</del>组成的整体(X<del>1</del>,X<del>2</del>…X<del>n</del>)被称为来自总体X且容量为n的一个简单随机样本，简称样本(一组随机变量，n个被抽取的灯泡)。一次抽样结果的n个具体数值(x<del>1</del>,x<del>2</del>…x<del>n</del>)称为对应样本X<del>1</del>、X<del>2</del>…X<del>n</del>的一个观测值(一组样本值)；</li>
<li>样本的分布：因为容量为n的样本(X<del>1</del>,X<del>2</del>…X<del>n</del>)中的n个随机变量独立同分布，因此有如定理成立</li>
</ul>
<p><img src="/images/image-20230515123544201.png" srcset="/img/loading.gif" lazyload></p>
<blockquote>
<p>数理统计这门学科就是利用抽取的样本(X<del>1</del>,X<del>2</del>…X<del>n</del>)对总体X的分布函数、数学期望、方差等做出估计；</p>
</blockquote>
<h2 id="2-统计量及其分布"><a href="#2-统计量及其分布" class="headerlink" title="2.统计量及其分布"></a>2.统计量及其分布</h2><h3 id="2-1-统计量的定义"><a href="#2-1-统计量的定义" class="headerlink" title="2.1 统计量的定义"></a>2.1 统计量的定义</h3><blockquote>
<p>统计量的定义：设X<del>1</del>,X<del>2</del>…X<del>n</del>是来自总体X的一个样本，g(x<del>1</del>,x<del>2</del>…x<del>n</del>)是n元函数，如果g中不包含任何未知参数，则称g(X<del>1</del>,X<del>2</del>…X<del>n</del>)是样本X<del>1</del>,X<del>2</del>…X<del>n</del>的一个统计量。若x<del>1</del>,x<del>2</del>…x<del>n</del>为样本值(观测值)，则称g(x<del>1</del>,x<del>2</del>…x<del>n</del>)是g(X<del>1</del>,X<del>2</del>…X<del>n</del>)的观测值；</p>
</blockquote>
<ul>
<li><p>数理统计中往往不会直接使用样本本身，通常针对不同问题来构造不同的样本的函数，该函数就被称为统计量；</p>
<ul>
<li>统计量能够提供有关总体参数的信息，并且在不同问题和场景下可以构造适当的统计量；</li>
<li>使用统计量而不直接使用样本本身有助于简化数据、减少随机性的影响、提供总体参数的估计和进行假设检验；</li>
<li>构造适当的统计量是为了更好地回答我们感兴趣的问题，并从样本数据中提取有用的信息；</li>
</ul>
</li>
<li><p>作为随机变量的函数，统计量也是一个随机变量（关于多维随机变量的函数，可参考[二维随机变量函数的分布](# 5.多维随机变量函数的分布)），因此统计量也存在分布，也就是之后会介绍的三大分布；</p>
</li>
</ul>
<h3 id="2-2-常用统计量"><a href="#2-2-常用统计量" class="headerlink" title="2.2 常用统计量"></a>2.2 常用统计量</h3><p>样本数字特征和顺序统计量都是常用的统计量（除了常见的统计量，根据具体问题和分析需求，还可以构造其他统计量来描述和分析数据）</p>
<h4 id="2-2-1-样本数字特征"><a href="#2-2-1-样本数字特征" class="headerlink" title="2.2.1 样本数字特征"></a>2.2.1 样本数字特征</h4><p>注意，总体X的数字特征如数学期望(总体均值)、方差和标准差、矩等都是概率论中的理想概念，这里介绍的样本数字特征是数理统计中的真实统计概念（因此在说均值或者方差的时候不加“样本”前缀一般都默认是总体的数字特征）。</p>
<p>设X<del>1</del>,X<del>2</del>…X<del>n</del>是来自总体X的简单随机样本，则相应的样本数字特征定义如下</p>
<p><img src="/images/image-20230515124627681.png" srcset="/img/loading.gif" lazyload></p>
<h4 id="2-2-2-顺序统计量"><a href="#2-2-2-顺序统计量" class="headerlink" title="2.2.2 顺序统计量"></a>2.2.2 顺序统计量</h4><p>将样本X<del>1</del>,X<del>2</del>…X<del>n</del>的n个观测值按照取值从小到大的顺序排列x<del>(1)</del>&lt;&#x3D;x<del>(2)</del>&lt;&#x3D;…&lt;&#x3D;x<del>(n)</del>，称随机变量X<del>(k)</del>为第k顺序统计量，其中X<del>(1)</del>是最小顺序统计量，X<del>(n)</del>是最大顺序统计量</p>
<p>最大值函数和最小值函数的一个重要考点是作为随机变量的函数的时候的分布，对应结论如下（常用于综合性大题）</p>
<p><img src="/images/image-20230515125030842.png" srcset="/img/loading.gif" lazyload></p>
<h4 id="2-2-3-常用统计量的性质"><a href="#2-2-3-常用统计量的性质" class="headerlink" title="2.2.3 常用统计量的性质"></a>2.2.3 常用统计量的性质</h4><p><img src="/images/image-20230515125131797.png" srcset="/img/loading.gif" lazyload></p>
<p>对上面的结论做进一步的解释。首先介绍无偏估计的概念<br>$$<br>若E(θ’)&#x3D;θ，则称θ’是θ的无偏估计<br>$$<br>已知总体X的期望为EX，方差为DX。样本的均值和方差分别是X^——^和S^2^。我们也可以利用无偏估计的概念联系总体和样本之间的数字特征</p>
<p><img src="/images/image-20230718164701976.png" srcset="/img/loading.gif" lazyload></p>
<h3 id="2-3-三大分布"><a href="#2-3-三大分布" class="headerlink" title="2.3 三大分布"></a>2.3 三大分布</h3><p>三大分布作为之后参数估计和假设检验的铺垫，均源自正态分布（这些分布都是概率论中的概念）。三大分布是数理统计中最常用的抽样分布（即统计量的分布，换句话说，统计量经常都服从这三大分布之一），这三大分布的概率密度无需记忆，只需要知道相应统计量的典型模式以及对应分布曲线的示意图和分位数即可。</p>
<h4 id="2-3-1-卡方分布"><a href="#2-3-1-卡方分布" class="headerlink" title="2.3.1 卡方分布"></a>2.3.1 卡方分布</h4><p>（1）典型模式</p>
<p><img src="/images/image-20230515130036646.png" srcset="/img/loading.gif" lazyload></p>
<ul>
<li>自由度是指和式中的独立变量的个数；</li>
<li>某分布的上α分位点(上侧α分位数)为μ<del>α</del>指的是：点μ<del>α</del>的右侧，概率密度曲线下方与x轴围成区域的面积为α；</li>
</ul>
<p>（2）性质</p>
<p><img src="/images/image-20230515130445967.png" srcset="/img/loading.gif" lazyload></p>
<h4 id="2-3-2-t分布"><a href="#2-3-2-t分布" class="headerlink" title="2.3.2 t分布"></a>2.3.2 t分布</h4><p>（1）典型模式</p>
<p><img src="/images/image-20230515130733741.png" srcset="/img/loading.gif" lazyload></p>
<p>（2）性质</p>
<p><img src="/images/image-20230515130753275.png" srcset="/img/loading.gif" lazyload></p>
<h4 id="2-3-3-F分布"><a href="#2-3-3-F分布" class="headerlink" title="2.3.3 F分布"></a>2.3.3 F分布</h4><p>（1）典型模式</p>
<p><img src="/images/image-20230515130835121.png" srcset="/img/loading.gif" lazyload></p>
<p>（2）性质</p>
<p><img src="/images/image-20230515130904628.png" srcset="/img/loading.gif" lazyload></p>
<h4 id="2-3-4-正态总体下的结论"><a href="#2-3-4-正态总体下的结论" class="headerlink" title="2.3.4 正态总体下的结论*"></a>2.3.4 正态总体下的结论*</h4><p>若X<del>1</del>,X<del>2</del>…X<del>n</del>是来自正态总体N(μ,σ^2^)的一个样本，X^——^,S^2^分别代表样本的均值和方差，则</p>
<p><img src="/images/image-20230515131458108.png" srcset="/img/loading.gif" lazyload>、</p>
<blockquote>
<ul>
<li>上述四个结论实际上是三大分布的应用，也是后面参数估计和假设检验的理论依据，直接看结论可能存在一定难度，下面给出推导过程</li>
</ul>
<p><img src="/images/image-20230807160644471.png" srcset="/img/loading.gif" lazyload></p>
</blockquote>
<h2 id="3-参数的点估计"><a href="#3-参数的点估计" class="headerlink" title="3.参数的点估计"></a>3.参数的点估计</h2><p>参考链接：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/104618189">统计学基础–点估计和区间估计 - 知乎 (zhihu.com)</a>，举个简单的例子点估计就是估计全校男生身高为1.75m，区间估计就是估计全校男生身高在1.70~1.80m之间；</p>
<hr>
<p>数理统计就是一门根据样本数据情况推断总体数据情况的学科，主要分为以下三种类型：</p>
<ul>
<li>样本均值–&gt;总体均值</li>
<li>样本方差–&gt;总体方差</li>
<li>样本比例–&gt;总体比例</li>
</ul>
<p>将根据样本统计量的数值对总体参数进行估计的过程称为<code>参数估计</code>，根据参数估计的性质的不同，将其分为如下两种类型：</p>
<ul>
<li><p>点估计：利用样本数据，对未知的参数进行估计所得到的一个具体的数据；</p>
<ul>
<li>常用样本均值估计总体均值，或用样本标准差估计总体标准差；</li>
<li>点估计不能提供估计参数的估计误差大小；</li>
</ul>
</li>
<li><p>区间估计：通过样本数据，估计未知参数，在可信度下的最可能的存在区间中得到的，结果是一个区间；</p>
<ul>
<li>常用样本比例估计总体比例；</li>
<li>区间估计就是在推断总体参数时，还要根据统计量的抽样分布特征，估计出总体参数的一个区间，而不是一个数值，并同时给出总体参数落在这一区间的可能性大小，概率的保证；</li>
</ul>
</li>
</ul>
<blockquote>
<p>点估计使用样本统计量来估计总体参数，因为样本统计量为数轴上某一点值，估计的结果也以一个点的数值表示，所以称为点估计。</p>
</blockquote>
<h3 id="3-1-概念"><a href="#3-1-概念" class="headerlink" title="3.1 概念"></a>3.1 概念</h3><p>设总体X的分布函数为F(x,θ)，其中θ是一个未知参数。X<del>1</del>,X<del>2</del>…X<del>n</del>是取自总体X的一个样本，由样本构造的一个统计量<code>θ^</code>(X<del>1</del>,X<del>2</del>…X<del>n</del>)作为参数θ的估计量，如果x<del>1</del>,x<del>2</del>…x<del>n</del>是样本的一个观测值，将其代入估计量<code>θ^</code>中称这个值为未知参数θ的估计值。</p>
<h4 id="3-1-1-矩估计法"><a href="#3-1-1-矩估计法" class="headerlink" title="3.1.1 矩估计法"></a>3.1.1 矩估计法</h4><p>矩估计法的特点是不需要知道总体X的分布，但这就导致不能充分利用已知的信息(比如已知总体分布是正态分布但是因为使用的是矩估计，所以这个信息不会被使用)，矩估计量不具备唯一性；</p>
<p>矩估计的基本思想就是利用样本均值去逼近总体均值，用样本方差估计总体方差（分一个未知参数和两个未知参数的情况）</p>
<p><img src="/images/image-20230807162209816.png" srcset="/img/loading.gif" lazyload></p>
<ul>
<li>因为点估计都是“估计”，故最终得到的结果不是精确值而是估计值，估计值一般带^与精确值做区分；</li>
<li>矩估计量中的样本均值大写，矩估计值中的样本均值小写；</li>
</ul>
<h4 id="3-1-2-最大似然估计"><a href="#3-1-2-最大似然估计" class="headerlink" title="3.1.2 最大似然估计"></a>3.1.2 最大似然估计</h4><p>基本思想是最大似然原理，即在未知参数θ取某个值的时候，整个样本获得观测值的概率最大的参数值<code>θ^</code>作为θ的估计。主要根据总体X是离散型还是连续型分为如下两个</p>
<p><img src="/images/image-20230807162521977.png" srcset="/img/loading.gif" lazyload></p>
<h3 id="3-2-估计量的评估"><a href="#3-2-估计量的评估" class="headerlink" title="3.2 估计量的评估"></a>3.2 估计量的评估</h3><p><img src="/images/image-20230515172106523.png" srcset="/img/loading.gif" lazyload></p>
<h2 id="4-参数的区间估计"><a href="#4-参数的区间估计" class="headerlink" title="4.参数的区间估计"></a>4.参数的区间估计</h2><h3 id="4-1-区间估计"><a href="#4-1-区间估计" class="headerlink" title="4.1 区间估计"></a>4.1 区间估计</h3><p>给定置信度求未知参数置信区间的问题，称为参数的区间估计问题（置信区间的长度表示估计的精度，置信区间越短表示估计的精度越高）。区间估计的推导过程如下（主要依赖前面介绍的[正态总体下的结论](# 2.3.4 正态总体下的结论* )）</p>
<p><img src="/images/image-20230807161322032.png" srcset="/img/loading.gif" lazyload></p>
<p>下面给出正态总体均值的置信区间表格（这个结论本质上硬背下来也没什么意义，重要的应该是理解上面那段推导）</p>
<p><img src="/images/image-20230515172524118.png" srcset="/img/loading.gif" lazyload></p>
<h3 id="4-2-假设检验"><a href="#4-2-假设检验" class="headerlink" title="4.2 假设检验"></a>4.2 假设检验</h3><p>区间估计与假设检验的关系非常密切，假设检验的推导如下（即小概率事件发生则拒绝整个H<del>0</del>原假设）</p>
<p><img src="/images/image-20230807161524986.png" srcset="/img/loading.gif" lazyload></p>
<p>下面给出正态总体下的六大检验及其拒绝域</p>
<p><img src="/images/image-20230515173216205.png" srcset="/img/loading.gif" lazyload></p>
<h3 id="4-3-两类错误"><a href="#4-3-两类错误" class="headerlink" title="4.3 两类错误"></a>4.3 两类错误</h3><p><img src="/images/image-20230515173243124.png" srcset="/img/loading.gif" lazyload></p>
<p>其中犯第一类错误的概率α就是前面所说的显著性水平α（但是在具体的考题中不会利用这个性质来求α，求解α和β都是直接利用公式计算条件概率），比如P{判定无病|有病}&#x3D;α，这个α一定要严格控制（有病但是不治疗后果很严重）；第二类错误可以认为是P{判定有病|无病}&#x3D;β，这个β尽量小即可（没有病多吃点药可能影响不大）</p>
<blockquote>
<ul>
<li>犯两类错误的概率α与β，并不满足β&#x3D;1-α，在固定样本容量n的条件下，α小，β就大；β小，α就大.在实际应用中，我们总是在控制α的条件下，尽量使β小，这是因为人们常常把拒绝H<del>0</del>比错误地接受H<del>0</del>看得更重要；</li>
<li>某些考题可能会更换措辞，针对这种题型只需要记住上述基本结论和下面的等价描述</li>
<li>拒绝H<del>0</del>&#x3D;&#x3D;接受H<del>1</del></li>
<li>接受H<del>0</del>&#x3D;&#x3D;拒绝H<del>1</del></li>
<li>H<del>0</del>为真&#x3D;&#x3D;H<del>1</del>为假</li>
<li>H<del>0</del>为假&#x3D;&#x3D;H<del>1</del>为真</li>
</ul>
</blockquote>

              
            </div>
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/%E8%80%83%E7%A0%94/" class="category-chain-item">考研</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/">#概率论与数理统计</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>考研_数学一_概率论</div>
      <div>https://gintoki-jpg.github.io/2023/03/02/考研_数学一_概率论/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>杨再俨</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2023年3月2日</div>
        </div>
      
      
      <div class="license-meta-item">
        <div>许可协议</div>
        <div>
          
            
            
              <a target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
              <span class="hint--top hint--rounded" aria-label="BY - 署名">
                <i class="iconfont icon-by"></i>
              </span>
              </a>
            
          
        </div>
      </div>
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2023/03/03/%E5%85%B6%E4%BB%96_%E5%A4%A7%E9%9B%81%E8%AF%AD%E6%B3%95/" title="六级_语法">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">六级_语法</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2023/03/01/%E5%B7%A5%E5%85%B7_pytorch/" title="PyTorch">
                        <span class="hidden-mobile">PyTorch</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
  <article id="comments" lazyload>
    

  </article>


          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  






    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
  
  

</div>


  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>







  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.18.2/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      headingSelector : CONFIG.toc.headingSelector || 'h1,h2,h3,h4,h5,h6',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      collapseDepth   : CONFIG.toc.collapseDepth || 0,
      scrollSmooth    : true,
      headingsOffset  : -boardTop
    });
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }
  });
</script>


  <script>
  (function() {
    var enableLang = CONFIG.code_language.enable && CONFIG.code_language.default;
    var enableCopy = CONFIG.copy_btn;
    if (!enableLang && !enableCopy) {
      return;
    }

    function getBgClass(ele) {
      return Fluid.utils.getBackgroundLightness(ele) >= 0 ? 'code-widget-light' : 'code-widget-dark';
    }

    var copyTmpl = '';
    copyTmpl += '<div class="code-widget">';
    copyTmpl += 'LANG';
    copyTmpl += '</div>';
    jQuery('.markdown-body pre').each(function() {
      var $pre = jQuery(this);
      if ($pre.find('code.mermaid').length > 0) {
        return;
      }
      if ($pre.find('span.line').length > 0) {
        return;
      }

      var lang = '';

      if (enableLang) {
        lang = CONFIG.code_language.default;
        if ($pre[0].children.length > 0 && $pre[0].children[0].classList.length >= 2 && $pre.children().hasClass('hljs')) {
          lang = $pre[0].children[0].classList[1];
        } else if ($pre[0].getAttribute('data-language')) {
          lang = $pre[0].getAttribute('data-language');
        } else if ($pre.parent().hasClass('sourceCode') && $pre[0].children.length > 0 && $pre[0].children[0].classList.length >= 2) {
          lang = $pre[0].children[0].classList[1];
          $pre.parent().addClass('code-wrapper');
        } else if ($pre.parent().hasClass('markdown-body') && $pre[0].classList.length === 0) {
          $pre.wrap('<div class="code-wrapper"></div>');
        }
        lang = lang.toUpperCase().replace('NONE', CONFIG.code_language.default);
      }
      $pre.append(copyTmpl.replace('LANG', lang).replace('code-widget">',
        getBgClass($pre[0]) + (enableCopy ? ' code-widget copy-btn" data-clipboard-snippet><i class="iconfont icon-copy"></i>' : ' code-widget">')));

      if (enableCopy) {
        Fluid.utils.createScript('https://lib.baomitu.com/clipboard.js/2.0.10/clipboard.min.js', function() {
          var clipboard = new window.ClipboardJS('.copy-btn', {
            target: function(trigger) {
              var nodes = trigger.parentNode.childNodes;
              for (var i = 0; i < nodes.length; i++) {
                if (nodes[i].tagName === 'CODE') {
                  return nodes[i];
                }
              }
            }
          });
          clipboard.on('success', function(e) {
            e.clearSelection();
            e.trigger.innerHTML = e.trigger.innerHTML.replace('icon-copy', 'icon-success');
            setTimeout(function() {
              e.trigger.innerHTML = e.trigger.innerHTML.replace('icon-success', 'icon-copy');
            }, 2000);
          });
        });
      }
    });
  })();
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
